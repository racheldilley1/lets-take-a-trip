{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#img load\n",
    "import requests\n",
    "from fake_useragent import UserAgent \n",
    "from io import BytesIO\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#img show\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "#preprocessing\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# models \n",
    "from keras.applications.vgg16 import VGG16 \n",
    "from keras.models import Model\n",
    "\n",
    "# clustering and dimension reduction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# color distributions\n",
    "import cv2\n",
    "import imutils\n",
    "import urllib.request as urllib2\n",
    "# import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img, model):\n",
    "    # get the feature vector\n",
    "    features = model.predict(img, use_multiprocessing=True)\n",
    "    return features\n",
    "\n",
    "def show_image(img):\n",
    "    \n",
    "    image = Image.open(img)\n",
    "    imgplot = plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# function that lets you view a cluster (based on identifier)        \n",
    "def view_cluster(cluster):\n",
    "    plt.figure(figsize = (25,25));\n",
    "    \n",
    "    # gets the list of filenames for a cluster\n",
    "    imgs = groups[cluster]\n",
    "    # only allow up to 30 images to be shown at a time\n",
    "    if len(imgs) > 30:\n",
    "        t = f\"Clipping cluster number {cluster} size from {len(imgs)} to 30\"\n",
    "        imgs =  random.sample(img_list, 30)\n",
    "    else:\n",
    "        t = f\"Cluster number {cluster}\"\n",
    "        \n",
    "    # plot each image in the cluster\n",
    "    plt.suptitle(t, fontsize=30)\n",
    "    for index, img in enumerate(imgs):\n",
    "        plt.subplot(10,10,index+1);\n",
    "\n",
    "        img_a = np.array(img)\n",
    "        plt.imshow(img_a)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_link_df = pd.read_pickle('attractions_img_links_df.pkl')\n",
    "att_loc_df = pd.read_pickle('attractions_loc_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download images and img array into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()\n",
    "headers = {'user-agent': ua.random}\n",
    "\n",
    "end =  len(img_link_df)\n",
    "img_list = []\n",
    "img_preprocessed_list = []\n",
    "attraction_img_list = []\n",
    "image_color_preprocess_list = []\n",
    "locations = []\n",
    "urls = []\n",
    "\n",
    "for x in range(0, end ):\n",
    "    \n",
    "    url_list = img_link_df.iloc[x]\n",
    "#     print(url_list)\n",
    "    att = img_link_df.iloc[[x]].index.values[0]\n",
    "#     print(att)\n",
    "    img_num = 1\n",
    "    for url in url_list:\n",
    "\n",
    "        try:\n",
    "            #download img from url\n",
    "            response = requests.get(url, headers = headers)\n",
    "            image_io = BytesIO(response.content)\n",
    "            img = image.load_img(image_io, target_size=(224, 224))\n",
    "            img_list.append(img)\n",
    "            \n",
    "            # convert from 'PIL.Image.Image' to numpy array\n",
    "            img_array = np.array(img)\n",
    "            img_array_reshaped = img_array.reshape(1,224,224,3) #reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "            img_preprocessed_list.append(preprocess_input(img_array_reshaped)) #prepare image for model\n",
    "            \n",
    "            #find name of attraction and img num in attraction\n",
    "            attraction_img_list.append(att+'_'+str(img_num))\n",
    "            img_num = img_num + 1\n",
    "            \n",
    "            urls.append(url)\n",
    "            \n",
    "            if img_num % 5 == 0:\n",
    "                ua = UserAgent()\n",
    "                headers = {'user-agent': ua.random}\n",
    "            \n",
    "#             #preprocess for color\n",
    "#             req = urllib.request.Request(url, headers=headers)\n",
    "#             resp = urllib.request.urlopen(req)\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "#             features = []\n",
    "#             try:\n",
    "#                 resp = urllib.request.urlopen(url)\n",
    "#                 image = np.array(bytearray(resp.read()), dtype = np.uint8)\n",
    "#                 image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "#                 image_color_preprocess_list.append(image)\n",
    "#             except:\n",
    "#                 print(\"xxxxxxx\")\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preprocessed_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(img_list), len(attraction_img_list), len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = VGG16()\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geat features of each image and load into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for name,img in zip(attraction_img_list, img_preprocessed_list):\n",
    "    data[name] = extract_features(img, model)\n",
    "    \n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "print(feat.shape)\n",
    "\n",
    "# reshape so that there are all samples of 4096 vectors\n",
    "feat = feat.reshape(-1,4096)\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction unsing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100, random_state=22)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "kmeans = KMeans(n_clusters=k,n_jobs=-1, random_state=22)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kmeans.labels_), len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holds the cluster id and the images { id: [images] }\n",
    "groups = {}\n",
    "for img, cluster in zip(img_list,kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(img)\n",
    "    else:\n",
    "        groups[cluster].append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(0,k):\n",
    "    view_cluster(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimal k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just incase you want to see which value for k might be the best \n",
    "sse = []\n",
    "list_k = list(range(25, 125))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, random_state=22, n_jobs=-1)\n",
    "    km.fit(x)\n",
    "    \n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse)\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, while RGB values are simple to understand, the RGB color space fails to mimic how humans perceive color. \n",
    "# Instead, we are going to use the HSV color space which maps pixel intensities into a cylinder:\n",
    "import time\n",
    "\n",
    "def load_img_open_cv(url):\n",
    "    ua = UserAgent()\n",
    "    headers = ('user-agent', ua.random)\n",
    "    \n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header(headers[0], headers[1])\n",
    "    resp = urllib2.urlopen(request)\n",
    "#     resp = urllib.request.urlopen(url)\n",
    "\n",
    "    image = np.array(bytearray(resp.read()), dtype = np.uint8)\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    return image\n",
    "    \n",
    "def describe( url, bins):\n",
    "    try:\n",
    "        image = load_img_open_cv(url)\n",
    "    #     print(image)\n",
    "\n",
    "        try:\n",
    "            # convert the image to the HSV color space and initialize the features used to quantify the image\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "            features = []\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        # grab the dimensions and compute the center of the image\n",
    "        (h, w) = image.shape[:2]\n",
    "        (cX, cY) = (int(w * 0.5), int(h * 0.5))\n",
    "\n",
    "        # divide the image into four rectangles/segments (top-left, top-right, bottom-right, bottom-left)\n",
    "        segments = [(0, cX, 0, cY), (cX, w, 0, cY), (cX, w, cY, h), (0, cX, cY, h)]\n",
    "\n",
    "        # construct an elliptical mask representing the center of the image\n",
    "        (axesX, axesY) = (int(w * 0.75) // 2, int(h * 0.75) // 2)\n",
    "        ellipMask = np.zeros(image.shape[:2], dtype = \"uint8\")\n",
    "        cv2.ellipse(ellipMask, (cX, cY), (axesX, axesY), 0, 0, 360, 255, -1)\n",
    "\n",
    "        # loop over the segments\n",
    "        for (startX, endX, startY, endY) in segments:\n",
    "\n",
    "            # construct a mask for each corner of the image, subtracting the elliptical center from it\n",
    "            cornerMask = np.zeros(image.shape[:2], dtype = \"uint8\")\n",
    "            cv2.rectangle(cornerMask, (startX, startY), (endX, endY), 255, -1)\n",
    "            cornerMask = cv2.subtract(cornerMask, ellipMask)\n",
    "\n",
    "            # extract a color histogram from the image, then update the feature vector\n",
    "            hist = histogram(image, cornerMask, bins)\n",
    "            features.extend(hist)\n",
    "\n",
    "        # extract a color histogram from the elliptical region and update the feature vector\n",
    "        hist = histogram(image, ellipMask, bins)\n",
    "        features.extend(hist)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def histogram(image, mask, bins):\n",
    "    # extract a 3D color histogram from the masked region of the image, using the supplied number of bins per channel\n",
    "    hist = cv2.calcHist([image], [0,1,2], mask, [bins,bins,bins],[0, 256, 0, 256, 0, 256])\n",
    "    \n",
    "    # normalize the histogram if we are using OpenCV 2.4\n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist).flatten()\n",
    "        \n",
    "    # otherwise handle for OpenCV 3+\n",
    "    else:\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_color_hist(url):\n",
    "    img = load_img_open_cv(url)\n",
    "    \n",
    "    color = ('b','g','r')\n",
    "    for i,col in enumerate(color):\n",
    "        histr = cv2.calcHist([img],[i],None,[256],[0,256])\n",
    "        plt.plot(histr,color = col)\n",
    "        plt.xlim([0,256])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Color features for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 7\n",
    "color_data = {}\n",
    "idx = 0\n",
    "for name,url in zip(attraction_img_list, urls):\n",
    "    color_data[name] = describe(url, bins)\n",
    "    \n",
    "#     if idx%5==0:\n",
    "#         ua = UserAgent()\n",
    "#         headers = ('user-agent', ua.random)\n",
    "    idx = idx +1\n",
    "    if idx == 200:\n",
    "        break\n",
    "    \n",
    "# get a list of just the color features\n",
    "color_feat = np.array(list(color_data.values()))\n",
    "print(color_feat.shape)\n",
    "\n",
    "# reshape so that there are all samples of 4096 vectors\n",
    "# feat = feat.reshape(-1,4096)\n",
    "# print(feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans with color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "kmeans_color = KMeans(n_clusters=k,n_jobs=-1, random_state=22)\n",
    "kmeans_color.fit(color_feat)\n",
    "\n",
    "# holds the cluster id and the images { id: [images] }\n",
    "color_groups = {}\n",
    "for img, cluster in zip(img_list,kmeans_color.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        color_groups[cluster] = []\n",
    "        color_groups[cluster].append(img)\n",
    "    else:\n",
    "        color_groups[cluster].append(img)\n",
    "        \n",
    "for cluster in range(0,k):\n",
    "    view_cluster(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_color.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(describe(urls[666], 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
