{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "#webscraping\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_df(state):\n",
    "    '''\n",
    "    A fucntion for scraping TripAdvisor to get all info for top 30 attractions for one state\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state as a string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df containing all top attractions and images for a state\n",
    "    '''\n",
    "    URL = \"https://www.tripadvisor.com/Attractions/\"\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)  #pause to be sure page has loaded\n",
    "    \n",
    "    #find state with search on main page\n",
    "    search = driver.find_element_by_name('q')\n",
    "    search.send_keys(state + \", United States\"  )\n",
    "    time.sleep(2)\n",
    "    search.send_keys(Keys.DOWN)\n",
    "    if state == 'Washington DC':\n",
    "        search.send_keys(Keys.DOWN)\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    # click Things to Do button\n",
    "    things_to_do_button = driver.find_elements_by_xpath('//*[contains(text() , \"Things to Do\")]')[0]\n",
    "    things_to_do_button.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    error = 0\n",
    "    try:\n",
    "        # click second See All button\n",
    "        see_all_button = driver.find_elements_by_xpath('//*[contains(text() , \"See all\")]')[1]\n",
    "        see_all_button.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        error = error + 1\n",
    "    \n",
    "    #check if second window appears, if yes switch back to main window and click see more button\n",
    "    try: \n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "        #click See more button\n",
    "        attraction_button = driver.find_elements_by_xpath('//*[contains(text() , \"See more\")]')[0]\n",
    "        attraction_button.click()\n",
    "        \n",
    "        #initalize BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"lxml\")\n",
    "    \n",
    "    except:\n",
    "        #initalize BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"lxml\")\n",
    "        \n",
    "    url = \"https://www.tripadvisor.com\"\n",
    "    links = {}\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "    \n",
    "    #loop through range to get numbers 1-30, we want top 30 attractions of each state, then for each number\n",
    "    #loop through links, add attraction name and link to links dictionary if number match outside loop\n",
    "    for x in range(1,31):\n",
    "        find = str(x) + \"[.] \"\n",
    "        for l in link_list:\n",
    "            if re.search(find,l.text):\n",
    "                name = l.text\n",
    "                name = re.sub(r'^.*? ', '', name) #clean name\n",
    "                links[name] = url + l['href']\n",
    "    \n",
    "    driver.quit() #close browser\n",
    "\n",
    "    return get_state_attraction_info(links, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_attraction_info(link_dict, state):\n",
    "    '''\n",
    "    A fucntion for scraping TripAdvisor to get locations and image links for top 30 attractions for one state\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dictionary, key is attraction name and value is link\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df containing all top attractions, locations, and images for a state\n",
    "    '''\n",
    "    df = pd.DataFrame(columns = ['name', 'location', 'img_num'])\n",
    "    idx = 0\n",
    "    img_link_dict = {}\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    headers = {'user-agent': ua.random}\n",
    "    \n",
    "    #loop through dictionary append info to df\n",
    "    for key, value in link_dict.items():\n",
    "\n",
    "        response = requests.get(value, headers = headers)\n",
    "        page= response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        time.sleep(1)  #pause to be sure page has loaded\n",
    "        \n",
    "        #try to find attraction location, return state name if none\n",
    "        try:\n",
    "            body = soup.find('div', {'data-tab' :'TABS_LOCATION'})\n",
    "            divs = body.find_all('span')\n",
    "            address = divs[3].text\n",
    "        except:\n",
    "            address = state\n",
    "            \n",
    "        img_link_list = get_photo_links(value)\n",
    "        img_link_dict[key] = img_link_list\n",
    "        \n",
    "        df = df.append(pd.DataFrame({'name':key, 'location': address, 'img_num' : len(img_link_list)}, \n",
    "                                    index=[idx]), ignore_index=True)\n",
    "        \n",
    "        idx = idx + 1\n",
    "        \n",
    "        #change user agent, chosen randomly every 3 loops\n",
    "        if idx%3 == 0:\n",
    "            headers = {'user-agent': ua.random}\n",
    "    \n",
    "    return (df,img_link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo_links(URL):\n",
    "    '''\n",
    "    A fucntion for scraping TripAdvisor to get all image links in a gallery for a specified attraction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url of attraction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list containing all image links for an attraction\n",
    "    '''\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)  #pause to be sure page has loaded\n",
    "\n",
    "    try:\n",
    "        #click All photos button\n",
    "        all_photos_button = driver.find_elements_by_xpath('//*[contains(text() , \"All photos\")]')[0]\n",
    "        all_photos_button.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    error = 0\n",
    "    try:\n",
    "        #click first photo\n",
    "        all_photos_button = driver.find_elements_by_class_name(\"photoGridImg\")[0]\n",
    "        all_photos_button.click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        error = error +1\n",
    "    \n",
    "    #initalize BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    images = soup.find_all('img') #find all imgs\n",
    "    \n",
    "    #loop through imgs, check for 2 image types, add to list if it is a TripAdvisor user uploaded photo\n",
    "    img_links = []\n",
    "    errors = 0\n",
    "    for i in images[:-1]:\n",
    "        try:\n",
    "            try:\n",
    "                link = i.attrs['data-lazyurl']\n",
    "                if re.search('photo',link):\n",
    "                    img_links.append(link)\n",
    "            except:\n",
    "                link = i.attrs['src']\n",
    "                if re.search('photo',link):\n",
    "                    img_links.append(link)\n",
    "        except:\n",
    "            errors = errors + 1\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return img_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top 30 attarctions for each state in continental US with location and 50+ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_df = pd.DataFrame(columns = ['name', 'location', 'img_num'])\n",
    "# img_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\n",
    "          \"Kansas\",\"Kentucky\", \"Louisiana\",\n",
    "          \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \"Montana\",\n",
    "          \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"Nevada\",\n",
    "          \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\",\n",
    "          \"Tennessee\",\"Texas\",\"Utah\",\"Virginia\",\"Vermont\",\"Washington\",\"Wisconsin\",\"West Virginia\", \"Wyoming\"]\n",
    "# \"Alabama\",\"Arkansas\", \"Arizona\",\"California\",\"Colorado\",\"Connecticut\", \"Washington DC\", \"Delaware\",\n",
    "# \"Florida\", \"Georgia\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \n",
    "\n",
    "for s in states:\n",
    "    df, images = get_state_df(s)\n",
    "#     print(images)\n",
    "#     print(df)\n",
    "    loc_df = pd.concat([loc_df, df], axis=0, ignore_index=True)\n",
    "    img_dict.update(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>img_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barber Vintage Motorsports Museum</td>\n",
       "      <td>6030 Barber Motorsports, Birmingham, AL 35094-...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rosa Parks Library and Museum</td>\n",
       "      <td>251 Montgomery St 251 Montgomery Street, 36104...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Orange Beach Welcome Center</td>\n",
       "      <td>23685 Perdido Beach Blvd, Orange Beach, AL 365...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. Space and Rocket Center</td>\n",
       "      <td>1 Tranquility Base, Huntsville, AL 35805-3371</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adventure Island</td>\n",
       "      <td>24559 Perdido Beach Blvd, Orange Beach, AL 365...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Antelope Flats</td>\n",
       "      <td>Jackson, WY</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>Medicine Bow National Forest</td>\n",
       "      <td>2468 Jackson St, Laramie, WY 82070-6535</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>Chief Joseph Scenic Highway</td>\n",
       "      <td>WY</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Grand Teton National Park</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>Antler Arches of Jackson</td>\n",
       "      <td>US Highway 26, Jackson, WY 83001</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1469 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name  \\\n",
       "0     Barber Vintage Motorsports Museum   \n",
       "1         Rosa Parks Library and Museum   \n",
       "2           Orange Beach Welcome Center   \n",
       "3          U.S. Space and Rocket Center   \n",
       "4                      Adventure Island   \n",
       "...                                 ...   \n",
       "1464                     Antelope Flats   \n",
       "1465       Medicine Bow National Forest   \n",
       "1466        Chief Joseph Scenic Highway   \n",
       "1467          Grand Teton National Park   \n",
       "1468           Antler Arches of Jackson   \n",
       "\n",
       "                                               location img_num  \n",
       "0     6030 Barber Motorsports, Birmingham, AL 35094-...      58  \n",
       "1     251 Montgomery St 251 Montgomery Street, 36104...      58  \n",
       "2     23685 Perdido Beach Blvd, Orange Beach, AL 365...      23  \n",
       "3         1 Tranquility Base, Huntsville, AL 35805-3371      58  \n",
       "4     24559 Perdido Beach Blvd, Orange Beach, AL 365...      58  \n",
       "...                                                 ...     ...  \n",
       "1464                                        Jackson, WY      58  \n",
       "1465            2468 Jackson St, Laramie, WY 82070-6535      58  \n",
       "1466                                                 WY      58  \n",
       "1467                                            Wyoming      58  \n",
       "1468                   US Highway 26, Jackson, WY 83001      58  \n",
       "\n",
       "[1469 rows x 3 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42690"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_dict)*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/attractions_loc_df.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-e752b149661e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Data/attractions_loc_df.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2861\u001b[0;31m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2862\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2863\u001b[0m             \u001b[0mBuffer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_f\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0m_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/attractions_loc_df.pkl'"
     ]
    }
   ],
   "source": [
    "loc_df.to_pickle('../Data/attractions_loc_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
